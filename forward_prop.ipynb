{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1) What is the purpose of forward propagation in a neural network?\\n\\nForward Propogation neural network is to compute the predicted output using weights and biases \\nits purpose is to predict the output \\nthere are totally three layers input , hidden and output layer .\\nLoss Calculation is also essential for forward propogation it calculates the difference between true output and predicted output . \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1) What is the purpose of forward propagation in a neural network?\n",
    "\n",
    "Forward Propogation neural network is to compute the predicted output using weights and biases \n",
    "its purpose is to predict the output \n",
    "there are totally three layers input , hidden and output layer .\n",
    "Loss Calculation is also essential for forward propogation it calculates the difference between true output and predicted output . \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\\n\\nIn a single layer of feedforward network there are multiple things acting\\n\\nWeights and Biases : weights are random number assigned to each neuron initially and bias is also the same\\n\\nActivation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\n",
    "\n",
    "In a single layer of feedforward network there are multiple things acting\n",
    "\n",
    "Weights and Biases : weights are random number assigned to each neuron initially and bias is also the same\n",
    "\n",
    "Activation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ3. How are activation functions used during forward propagation?\\n\\nActivation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\\n\\nthere are multiple \\n\\nBias : \\nBiases shift the activation function, allowing neurons to activate even when the input is zero or small.\\nThey help prevent \"dead neurons\" in functions like ReLU and improve the model\\'s flexibility.\\nBias is analogous to the intercept in linear models, making the network more expressive.\\nBiases are learned during training and are crucial for accurate prediction and pattern recognition.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q3. How are activation functions used during forward propagation?\n",
    "\n",
    "Activation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\n",
    "\n",
    "there are multiple \n",
    "\n",
    "Bias : \n",
    "Biases shift the activation function, allowing neurons to activate even when the input is zero or small.\n",
    "They help prevent \"dead neurons\" in functions like ReLU and improve the model's flexibility.\n",
    "Bias is analogous to the intercept in linear models, making the network more expressive.\n",
    "Biases are learned during training and are crucial for accurate prediction and pattern recognition.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ5. What is the purpose of applying a softmax function in the output layer during forward propagation?\\n\\nSoftmax function in the output layer helps us to classify multiple classification by calculating probability for each\\npossible output\\nand the total probability value for all the classes would be 1\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?\n",
    "\n",
    "Softmax function in the output layer helps us to classify multiple classification by calculating probability for each\n",
    "possible output\n",
    "and the total probability value for all the classes would be 1\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQ6. What is the purpose of backward propagation in a neural network?\\nBackward propogation is used to adjust the weight's and biases after calculating the loss function \\nweights and biases are updated during the process of backward propogation\\nBackpropogatoin calculates gradient using partial derivative of loss function \\nweights and biases are updated using the gradient descendant function \\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q6. What is the purpose of backward propagation in a neural network?\n",
    "Backward propogation is used to adjust the weight's and biases after calculating the loss function \n",
    "weights and biases are updated during the process of backward propogation\n",
    "Backpropogatoin calculates gradient using partial derivative of loss function \n",
    "weights and biases are updated using the gradient descendant function \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git_pw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
