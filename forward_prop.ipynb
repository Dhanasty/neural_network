{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1) What is the purpose of forward propagation in a neural network?\\n\\nForward Propogation neural network is to compute the predicted output using weights and biases \\nits purpose is to predict the output \\nthere are totally three layers input , hidden and output layer .\\nLoss Calculation is also essential for forward propogation it calculates the difference between true output and predicted output . \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1) What is the purpose of forward propagation in a neural network?\n",
    "\n",
    "Forward Propogation neural network is to compute the predicted output using weights and biases \n",
    "its purpose is to predict the output \n",
    "there are totally three layers input , hidden and output layer .\n",
    "Loss Calculation is also essential for forward propogation it calculates the difference between true output and predicted output . \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\\n\\nIn a single layer of feedforward network there are multiple things acting\\n\\nWeights and Biases : weights are random number assigned to each neuron initially and bias is also the same\\n\\nActivation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\n",
    "\n",
    "In a single layer of feedforward network there are multiple things acting\n",
    "\n",
    "Weights and Biases : weights are random number assigned to each neuron initially and bias is also the same\n",
    "\n",
    "Activation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ3. How are activation functions used during forward propagation?\\n\\nActivation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\\n\\nthere are multiple \\n\\nBias : \\nBiases shift the activation function, allowing neurons to activate even when the input is zero or small.\\nThey help prevent \"dead neurons\" in functions like ReLU and improve the model\\'s flexibility.\\nBias is analogous to the intercept in linear models, making the network more expressive.\\nBiases are learned during training and are crucial for accurate prediction and pattern recognition.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q3. How are activation functions used during forward propagation?\n",
    "\n",
    "Activation function : it is introduced to bring non linearity into the model. which ensures which neuron to fire and not fire\n",
    "\n",
    "there are multiple \n",
    "\n",
    "Bias : \n",
    "Biases shift the activation function, allowing neurons to activate even when the input is zero or small.\n",
    "They help prevent \"dead neurons\" in functions like ReLU and improve the model's flexibility.\n",
    "Bias is analogous to the intercept in linear models, making the network more expressive.\n",
    "Biases are learned during training and are crucial for accurate prediction and pattern recognition.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ5. What is the purpose of applying a softmax function in the output layer during forward propagation?\\n\\nSoftmax function in the output layer helps us to classify multiple classification by calculating probability for each\\npossible output\\nand the total probability value for all the classes would be 1\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?\n",
    "\n",
    "Softmax function in the output layer helps us to classify multiple classification by calculating probability for each\n",
    "possible output\n",
    "and the total probability value for all the classes would be 1\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQ6. What is the purpose of backward propagation in a neural network?\\nBackward propogation is used to adjust the weight's and biases after calculating the loss function \\nweights and biases are updated during the process of backward propogation\\nBackpropogatoin calculates gradient using partial derivative of loss function \\nweights and biases are updated using the gradient descendant function \\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q6. What is the purpose of backward propagation in a neural network?\n",
    "Backward propogation is used to adjust the weight's and biases after calculating the loss function \n",
    "weights and biases are updated during the process of backward propogation\n",
    "Backpropogatoin calculates gradient using partial derivative of loss function \n",
    "weights and biases are updated using the gradient descendant function \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLetâ€™s break down how backward propagation is mathematically calculated in a single-layer feedforward neural network in a simple, step-by-step manner.\\n\\nWhat Is a Single-Layer Feedforward Neural Network?\\nA single-layer feedforward neural network has:\\n\\nInputs: \\nğ‘¥\\n=\\n[\\nğ‘¥\\n1\\n,\\nğ‘¥\\n2\\n,\\n.\\n.\\n.\\n,\\nğ‘¥\\nğ‘›\\n]\\nx=[x \\n1\\n\\u200b\\n ,x \\n2\\n\\u200b\\n ,...,x \\nn\\n\\u200b\\n ]\\nWeights: \\nğ‘¤\\n=\\n[\\nğ‘¤\\n1\\n,\\nğ‘¤\\n2\\n,\\n.\\n.\\n.\\n,\\nğ‘¤\\nğ‘›\\n]\\nw=[w \\n1\\n\\u200b\\n ,w \\n2\\n\\u200b\\n ,...,w \\nn\\n\\u200b\\n ]\\nBias: \\nğ‘\\nb\\nActivation function: Often, we use an activation function like sigmoid or ReLU.\\nThe output of the network is computed as:\\n\\nğ‘§\\n=\\nğ‘¤\\nğ‘‡\\nğ‘¥\\n+\\nğ‘\\n=\\nğ‘¤\\n1\\nğ‘¥\\n1\\n+\\nğ‘¤\\n2\\nğ‘¥\\n2\\n+\\nâ‹¯\\n+\\nğ‘¤\\nğ‘›\\nğ‘¥\\nğ‘›\\n+\\nğ‘\\nz=w \\nT\\n x+b=w \\n1\\n\\u200b\\n x \\n1\\n\\u200b\\n +w \\n2\\n\\u200b\\n x \\n2\\n\\u200b\\n +â‹¯+w \\nn\\n\\u200b\\n x \\nn\\n\\u200b\\n +b\\nThen the activation function is applied to get the final output:\\n\\nğ‘¦\\n=\\nğ‘“\\n(\\nğ‘§\\n)\\ny=f(z)\\nWhere \\nğ‘“\\n(\\nğ‘§\\n)\\nf(z) can be, for example, the sigmoid activation:\\n\\nğ‘“\\n(\\nğ‘§\\n)\\n=\\n1\\n1\\n+\\nğ‘’\\nâˆ’\\nğ‘§\\nf(z)= \\n1+e \\nâˆ’z\\n \\n1\\n\\u200b\\n \\nGoal of Backpropagation\\nThe goal of backpropagation is to adjust the weights \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n  and bias \\nğ‘\\nb to minimize the error between the predicted output \\nğ‘¦\\ny and the true value \\nğ‘¦\\ntrue\\ny \\ntrue\\n\\u200b\\n .\\n\\nStep-by-Step Calculation of Backpropagation\\nForward Propagation: Start by calculating the predicted output \\nğ‘¦\\ny from the input \\nğ‘¥\\nx using the current weights and bias.\\n\\nLoss Function: Define a loss function to measure the error between the predicted output \\nğ‘¦\\ny and the true value \\nğ‘¦\\ntrue\\ny \\ntrue\\n\\u200b\\n .\\n\\nFor simplicity, weâ€™ll use the Mean Squared Error (MSE) as the loss function:\\n\\nğ¿\\n=\\n1\\n2\\n(\\nğ‘¦\\nâˆ’\\nğ‘¦\\ntrue\\n)\\n2\\nL= \\n2\\n1\\n\\u200b\\n (yâˆ’y \\ntrue\\n\\u200b\\n ) \\n2\\n \\nThe purpose is to minimize \\nğ¿\\nL, so we need to calculate how to change the weights \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n  and bias \\nğ‘\\nb to reduce \\nğ¿\\nL.\\n\\nCalculate Gradients (Using the Chain Rule): This is the key part of backpropagation. We calculate how much each weight \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n  and bias \\nğ‘\\nb contributed to the error, and then we adjust them. To do this, we use derivatives.\\nGradient of the Loss with Respect to the Output \\nğ‘¦\\ny:\\nWe first calculate how much the output \\nğ‘¦\\ny affects the loss \\nğ¿\\nL:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¦\\n=\\nğ‘¦\\nâˆ’\\nğ‘¦\\ntrue\\nâˆ‚y\\nâˆ‚L\\n\\u200b\\n =yâˆ’y \\ntrue\\n\\u200b\\n \\nThis is just the difference between the predicted output and the true value. If the prediction is far off, this value will be large.\\n\\nGradient of the Output with Respect to \\nğ‘§\\nz (Pre-Activation):\\nNext, we calculate how the output \\nğ‘¦\\ny changes with respect to the input to the activation function \\nğ‘§\\n=\\nğ‘¤\\nğ‘‡\\nğ‘¥\\n+\\nğ‘\\nz=w \\nT\\n x+b.\\n\\nFor a sigmoid activation function:\\n\\nâˆ‚\\nğ‘¦\\nâˆ‚\\nğ‘§\\n=\\nğ‘¦\\nâ‹…\\n(\\n1\\nâˆ’\\nğ‘¦\\n)\\nâˆ‚z\\nâˆ‚y\\n\\u200b\\n =yâ‹…(1âˆ’y)\\nThis derivative tells us how sensitive the output is to changes in the weighted sum \\nğ‘§\\nz.\\n\\nGradient of \\nğ‘§\\nz with Respect to the Weights \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n :\\nNow, we calculate how the weighted sum \\nğ‘§\\nz changes with respect to each weight \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n . Since \\nğ‘§\\n=\\nğ‘¤\\n1\\nğ‘¥\\n1\\n+\\nğ‘¤\\n2\\nğ‘¥\\n2\\n+\\nâ‹¯\\n+\\nğ‘¤\\nğ‘›\\nğ‘¥\\nğ‘›\\n+\\nğ‘\\nz=w \\n1\\n\\u200b\\n x \\n1\\n\\u200b\\n +w \\n2\\n\\u200b\\n x \\n2\\n\\u200b\\n +â‹¯+w \\nn\\n\\u200b\\n x \\nn\\n\\u200b\\n +b, the derivative of \\nğ‘§\\nz with respect to a weight \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n  is just the input \\nğ‘¥\\nğ‘–\\nx \\ni\\n\\u200b\\n :\\n\\nâˆ‚\\nğ‘§\\nâˆ‚\\nğ‘¤\\nğ‘–\\n=\\nğ‘¥\\nğ‘–\\nâˆ‚w \\ni\\n\\u200b\\n \\nâˆ‚z\\n\\u200b\\n =x \\ni\\n\\u200b\\n \\nGradient of \\nğ‘§\\nz with Respect to the Bias \\nğ‘\\nb:\\nSimilarly, the derivative of \\nğ‘§\\nz with respect to the bias \\nğ‘\\nb is simply:\\n\\nâˆ‚\\nğ‘§\\nâˆ‚\\nğ‘\\n=\\n1\\nâˆ‚b\\nâˆ‚z\\n\\u200b\\n =1\\nFull Gradient for the Weights:\\nNow we can use the chain rule to combine these results and find the full derivative of the loss \\nğ¿\\nL with respect to each weight \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n . The chain rule tells us to multiply the partial derivatives step by step:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¤\\nğ‘–\\n=\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¦\\nâ‹…\\nâˆ‚\\nğ‘¦\\nâˆ‚\\nğ‘§\\nâ‹…\\nâˆ‚\\nğ‘§\\nâˆ‚\\nğ‘¤\\nğ‘–\\nâˆ‚w \\ni\\n\\u200b\\n \\nâˆ‚L\\n\\u200b\\n = \\nâˆ‚y\\nâˆ‚L\\n\\u200b\\n â‹… \\nâˆ‚z\\nâˆ‚y\\n\\u200b\\n â‹… \\nâˆ‚w \\ni\\n\\u200b\\n \\nâˆ‚z\\n\\u200b\\n \\nSubstituting the values we derived earlier:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¤\\nğ‘–\\n=\\n(\\nğ‘¦\\nâˆ’\\nğ‘¦\\ntrue\\n)\\nâ‹…\\nğ‘¦\\nâ‹…\\n(\\n1\\nâˆ’\\nğ‘¦\\n)\\nâ‹…\\nğ‘¥\\nğ‘–\\nâˆ‚w \\ni\\n\\u200b\\n \\nâˆ‚L\\n\\u200b\\n =(yâˆ’y \\ntrue\\n\\u200b\\n )â‹…yâ‹…(1âˆ’y)â‹…x \\ni\\n\\u200b\\n \\nThis tells us how much to adjust each weight \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n .\\n\\nFull Gradient for the Bias:\\nSimilarly, the derivative of the loss \\nğ¿\\nL with respect to the bias \\nğ‘\\nb is:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘\\n=\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¦\\nâ‹…\\nâˆ‚\\nğ‘¦\\nâˆ‚\\nğ‘§\\nâ‹…\\nâˆ‚\\nğ‘§\\nâˆ‚\\nğ‘\\nâˆ‚b\\nâˆ‚L\\n\\u200b\\n = \\nâˆ‚y\\nâˆ‚L\\n\\u200b\\n â‹… \\nâˆ‚z\\nâˆ‚y\\n\\u200b\\n â‹… \\nâˆ‚b\\nâˆ‚z\\n\\u200b\\n \\nSubstitute the values:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘\\n=\\n(\\nğ‘¦\\nâˆ’\\nğ‘¦\\ntrue\\n)\\nâ‹…\\nğ‘¦\\nâ‹…\\n(\\n1\\nâˆ’\\nğ‘¦\\n)\\nâ‹…\\n1\\nâˆ‚b\\nâˆ‚L\\n\\u200b\\n =(yâˆ’y \\ntrue\\n\\u200b\\n )â‹…yâ‹…(1âˆ’y)â‹…1\\nThis gives us how much to adjust the bias.\\n\\n4. Updating the Weights and Bias\\nOnce we have the gradients, we use them to update the weights and bias in the direction that reduces the error. This is done using gradient descent.\\n\\nFor each weight \\nğ‘¤\\nğ‘–\\nw \\ni\\n\\u200b\\n , the update rule is:\\n\\nğ‘¤\\nğ‘–\\nnew\\n=\\nğ‘¤\\nğ‘–\\nold\\nâˆ’\\nğœ‚\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¤\\nğ‘–\\nw \\ni\\nnew\\n\\u200b\\n =w \\ni\\nold\\n\\u200b\\n âˆ’Î· \\nâˆ‚w \\ni\\n\\u200b\\n \\nâˆ‚L\\n\\u200b\\n \\nWhere \\nğœ‚\\nÎ· is the learning rate (a small constant that controls how big the weight updates are).\\n\\nFor the bias, the update rule is:\\n\\nğ‘\\nnew\\n=\\nğ‘\\nold\\nâˆ’\\nğœ‚\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘\\nb \\nnew\\n =b \\nold\\n âˆ’Î· \\nâˆ‚b\\nâˆ‚L\\n\\u200b\\n \\nExample\\nLetâ€™s go through a quick example with actual numbers to see this in action:\\n\\nInput \\nğ‘¥\\n=\\n[\\n1\\n,\\n0.5\\n]\\nx=[1,0.5]\\nWeights \\nğ‘¤\\n=\\n[\\n0.8\\n,\\n0.5\\n]\\nw=[0.8,0.5]\\nBias \\nğ‘\\n=\\n0.1\\nb=0.1\\nTrue label \\nğ‘¦\\ntrue\\n=\\n1\\ny \\ntrue\\n\\u200b\\n =1\\nLearning rate \\nğœ‚\\n=\\n0.1\\nÎ·=0.1\\nForward Propagation:\\nCalculate \\nğ‘§\\nz:\\n\\nğ‘§\\n=\\n0.8\\nâ‹…\\n1\\n+\\n0.5\\nâ‹…\\n0.5\\n+\\n0.1\\n=\\n1.15\\nz=0.8â‹…1+0.5â‹…0.5+0.1=1.15\\nApply the activation function (Sigmoid):\\n\\nğ‘¦\\n=\\n1\\n1\\n+\\nğ‘’\\nâˆ’\\n1.15\\nâ‰ˆ\\n0.759\\ny= \\n1+e \\nâˆ’1.15\\n \\n1\\n\\u200b\\n â‰ˆ0.759\\nBackward Propagation:\\nCalculate the loss:\\n\\nğ¿\\n=\\n1\\n2\\n(\\n0.759\\nâˆ’\\n1\\n)\\n2\\n=\\n0.029\\nL= \\n2\\n1\\n\\u200b\\n (0.759âˆ’1) \\n2\\n =0.029\\nCompute gradients:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¦\\n=\\n0.759\\nâˆ’\\n1\\n=\\nâˆ’\\n0.241\\nâˆ‚y\\nâˆ‚L\\n\\u200b\\n =0.759âˆ’1=âˆ’0.241\\nâˆ‚\\nğ‘¦\\nâˆ‚\\nğ‘§\\n=\\n0.759\\nâ‹…\\n(\\n1\\nâˆ’\\n0.759\\n)\\n=\\n0.182\\nâˆ‚z\\nâˆ‚y\\n\\u200b\\n =0.759â‹…(1âˆ’0.759)=0.182\\nFor weight \\nğ‘¤\\n1\\nw \\n1\\n\\u200b\\n :\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¤\\n1\\n=\\nâˆ’\\n0.241\\nâ‹…\\n0.182\\nâ‹…\\n1\\n=\\nâˆ’\\n0.0438\\nâˆ‚w \\n1\\n\\u200b\\n \\nâˆ‚L\\n\\u200b\\n =âˆ’0.241â‹…0.182â‹…1=âˆ’0.0438\\nFor weight \\nğ‘¤\\n2\\nw \\n2\\n\\u200b\\n :\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘¤\\n2\\n=\\nâˆ’\\n0.241\\nâ‹…\\n0.182\\nâ‹…\\n0.5\\n=\\nâˆ’\\n0.0219\\nâˆ‚w \\n2\\n\\u200b\\n \\nâˆ‚L\\n\\u200b\\n =âˆ’0.241â‹…0.182â‹…0.5=âˆ’0.0219\\nFor the bias:\\n\\nâˆ‚\\nğ¿\\nâˆ‚\\nğ‘\\n=\\nâˆ’\\n0.241\\nâ‹…\\n0.182\\nâ‹…\\n1\\n=\\nâˆ’\\n0.0438\\nâˆ‚b\\nâˆ‚L\\n\\u200b\\n =âˆ’0.241â‹…0.182â‹…1=âˆ’0.0438\\nUpdate the weights and bias:\\n\\nğ‘¤\\n1\\nnew\\n=\\n0.8\\nâˆ’\\n0.1\\nâ‹…\\n(\\nâˆ’\\n0.0438\\n)\\n=\\n0.8044\\nw \\n1\\nnew\\n\\u200b\\n =0.8âˆ’0.1â‹…(âˆ’0.0438)=0.8044\\nğ‘¤\\n2\\nnew\\n=\\n0.5\\nâˆ’\\n0.1\\nâ‹…\\n(\\nâˆ’\\n0.0219\\n)\\n=\\n0.5022\\nw \\n2\\nnew\\n\\u200b\\n =0.5âˆ’0.1â‹…(âˆ’0.0219)=0.5022\\nğ‘\\nnew\\n=\\n0.1\\nâˆ’\\n0.1\\nâ‹…\\n(\\nâˆ’\\n0.0438\\n)\\n=\\n0.1044\\nb \\nnew\\n =0.1âˆ’0.1â‹…(âˆ’0.0438)=0.1044\\nRecap:\\nForward propagation computes the output.\\nThe loss measures how far off the prediction is from the true value.\\nBackpropagation calculates the gradients (how much each weight and bias contributed to the error).\\nGradient descent updates the weights and bias to reduce the error for the next prediction.\\nBy repeating this process, the network learns to make better predictions.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git_pw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
