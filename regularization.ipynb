{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is regularization in the context of deep learning? Why is it important?\\n\\nRegularization referes to set of techniques reduce overfitting by adding penalties to the model and output\\n\\nregularization helps improve the models ability to generalize by adding constraints or penalties to the learning model\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "What is regularization in the context of deep learning? Why is it important?\n",
    "\n",
    "Regularization referes to set of techniques reduce overfitting by adding penalties to the model and output\n",
    "\n",
    "regularization helps improve the models ability to generalize by adding constraints or penalties to the learning model\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nEk Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoffk\\n\\ntwo types of errors an model can make is bias and variance\\n\\nBias : introducing a complex problem to a simple model . which can cause underfitting causing high bias \\n        model with high bias leads to fail when a complex problem comes\\n\\nVariance : refers to models sensitivity with the data . a small noise can impact large difference in the model\\n            leads to overfitting\\n\\nIN machine learning reducing bias typically increaes variance\\n\\nLow bias , High Variance : the model fits well with training data, but when a new problem comes the model struggles\\n\\nhigh bias, low variance : the model is simple and misses patters in the training data but generalizes poorly\\n\\n\\nBy adding regularization , the models' weights are penalized , preventing them from overfitting\\n\\nRegularization encourages the model to learn simpler\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Ek Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoffk\n",
    "\n",
    "two types of errors an model can make is bias and variance\n",
    "\n",
    "Bias : introducing a complex problem to a simple model . which can cause underfitting causing high bias \n",
    "        model with high bias leads to fail when a complex problem comes\n",
    "\n",
    "Variance : refers to models sensitivity with the data . a small noise can impact large difference in the model\n",
    "            leads to overfitting\n",
    "\n",
    "IN machine learning reducing bias typically increaes variance\n",
    "\n",
    "Low bias , High Variance : the model fits well with training data, but when a new problem comes the model struggles\n",
    "\n",
    "high bias, low variance : the model is simple and misses patters in the training data but generalizes poorly\n",
    "\n",
    "\n",
    "By adding regularization , the models' weights are penalized , preventing them from overfitting\n",
    "\n",
    "Regularization encourages the model to learn simpler\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and\\ntheir effects on the modelG\\n\\nL1 and L2 both are used to reduce overfitting in the machine learning models\\nBoth add penalty to loss function but they differ in how this penalty is calculated\\n\\n\\n\\nL1 (Lasso):\\n\\nForces some weights to zero.\\nSimplifies the model by ignoring some features.\\nGreat for when you think only a few features are really important.\\nL2 (Ridge):\\n\\nShrinks all weights but doesn’t make them zero.\\nKeeps all features in the model but makes their influence smaller.\\nUseful when you think all features have some role to play, but you don’t want any of them to dominate.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and\n",
    "their effects on the modelG\n",
    "\n",
    "L1 and L2 both are used to reduce overfitting in the machine learning models\n",
    "Both add penalty to loss function but they differ in how this penalty is calculated\n",
    "\n",
    "\n",
    "\n",
    "L1 (Lasso):\n",
    "\n",
    "Forces some weights to zero.\n",
    "Simplifies the model by ignoring some features.\n",
    "Great for when you think only a few features are really important.\n",
    "L2 (Ridge):\n",
    "\n",
    "Shrinks all weights but doesn’t make them zero.\n",
    "Keeps all features in the model but makes their influence smaller.\n",
    "Useful when you think all features have some role to play, but you don’t want any of them to dominate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on\\nmodel training and inference ?\\n\\nDropout Regularization is a technique to reduce overfiting by disableing random set of neurons on each training. this enables model to learn more and\\ngeneralized features rather than relying on too heavily on specific neurons\\n\\nDuring Training \\n\\nAt each trainign step the model drops or disables a percantage of neurons in a layer\\n\\nthe network must adjust to make the predictions without the help of dropped out neurons\\n\\nImpact of Dropout :\\n\\nSlower Convergence\\nImproved Generalizatoin\\nIncreased Stochasity\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on\n",
    "model training and inference ?\n",
    "\n",
    "Dropout Regularization is a technique to reduce overfiting by disableing random set of neurons on each training. this enables model to learn more and\n",
    "generalized features rather than relying on too heavily on specific neurons\n",
    "\n",
    "During Training \n",
    "\n",
    "At each trainign step the model drops or disables a percantage of neurons in a layer\n",
    "\n",
    "the network must adjust to make the predictions without the help of dropped out neurons\n",
    "\n",
    "Impact of Dropout :\n",
    "\n",
    "Slower Convergence\n",
    "Improved Generalizatoin\n",
    "Increased Stochasity\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git_pw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
