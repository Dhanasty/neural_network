{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is an activation function in the context of artificial neural networks?\\n\\nAn activation function in a neural network is a mathematical function applied to the output of a neuron (or node) \\nto introduce non-linearity into the model. This non-linearity allows the neural network to learn and model more complex patterns\\n and relationships in the data. Without activation functions, a neural network would essentially behave like a linear regression model,\\nmaking it unable to solve complex problems such as image recognition, language processing, and more.\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "An activation function in a neural network is a mathematical function applied to the output of a neuron (or node) \n",
    "to introduce non-linearity into the model. This non-linearity allows the neural network to learn and model more complex patterns\n",
    " and relationships in the data. Without activation functions, a neural network would essentially behave like a linear regression model,\n",
    "making it unable to solve complex problems such as image recognition, language processing, and more.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2. What are some common types of activation functions used in neural networks?\\n\\n1. Sigmoid \\n2. ReLu\\n3. Leaky relu\\n4. tanh \\n5.softmax \\n6. parameter ReLU\\n\\n\\nQ3. How do activation functions affect the training process and performance of a neural network?\\n\\nActivation functions play a crucial role in the training process and performance of a neural network by introducing non-linearity, allowing the network to learn complex patterns and relationships within data, which would be impossible without them; essentially, they determine which neurons \"fire\" and pass information on to the next layer, significantly impacting the network\\'s ability to generalize and make accurate predictions on new data. \\n\\nQ4. How does the sigmoid activation function work? What are its advantages and disadvantages?\\n    It is a squishing function it squishes value from 0 to 1 \\n    used in binary classfication problem\\n    \\n    Advantage :\\n        The sigmoid function outputs values between 0 and 1, which can be interpreted as probabilities. This is especially useful in binary classification problems where you want to model probabilities.\\n\\n    Disadvantages:\\n        Vanishing gradient \\n        non zero centered\\n        \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "1. Sigmoid \n",
    "2. ReLu\n",
    "3. Leaky relu\n",
    "4. tanh \n",
    "5.softmax \n",
    "6. parameter ReLU\n",
    "\n",
    "\n",
    "Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "Activation functions play a crucial role in the training process and performance of a neural network by introducing non-linearity, allowing the network to learn complex patterns and relationships within data, which would be impossible without them; essentially, they determine which neurons \"fire\" and pass information on to the next layer, significantly impacting the network's ability to generalize and make accurate predictions on new data. \n",
    "\n",
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "    It is a squishing function it squishes value from 0 to 1 \n",
    "    used in binary classfication problem\n",
    "    \n",
    "    Advantage :\n",
    "        The sigmoid function outputs values between 0 and 1, which can be interpreted as probabilities. This is especially useful in binary classification problems where you want to model probabilities.\n",
    "\n",
    "    Disadvantages:\n",
    "        Vanishing gradient \n",
    "        non zero centered\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\\n\\nThis function outputs the input directly if it is positive; otherwise, it will output zero.\\nUsed in hidden layers in Nueral network\\nReLU is favored for hidden layers in deep networks due to its efficiency and ability to handle the vanishing gradient problem effectively. It produces outputs that are either zero or positive, which helps in faster convergence and better training dynamics. On the other hand, Sigmoid is more suited for scenarios where output probabilities are needed, such as binary classification, but may suffer from vanishing gradients and slow convergence when used in deep networks.\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "This function outputs the input directly if it is positive; otherwise, it will output zero.\n",
    "Used in hidden layers in Nueral network\n",
    "ReLU is favored for hidden layers in deep networks due to its efficiency and ability to handle the vanishing gradient problem effectively. It produces outputs that are either zero or positive, which helps in faster convergence and better training dynamics. On the other hand, Sigmoid is more suited for scenarios where output probabilities are needed, such as binary classification, but may suffer from vanishing gradients and slow convergence when used in deep networks.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ6. What are the benefits of using the ReLU activation function over the sigmoid function?\\n\\nVanishing gradient\\nFaster convergence\\nrelu is used cnn , rnn\\ncomputational efficiency\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "Vanishing gradient\n",
    "Faster convergence\n",
    "relu is used cnn , rnn\n",
    "computational efficiency\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\\nLeaky Rectified Linear Unit (ReLU) is an activation function in neural networks that helps reduce the risk of overfitting. It\\'s based on the ReLU function, but instead of having a flat slope for negative values, Leaky ReLU has a small slope. This small gradient for negative inputs helps preserve some activity in the neurons.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "Leaky Rectified Linear Unit (ReLU) is an activation function in neural networks that helps reduce the risk of overfitting. It's based on the ReLU function, but instead of having a flat slope for negative values, Leaky ReLU has a small slope. This small gradient for negative inputs helps preserve some activity in the neurons.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q8. What is the purpose of the softmax activation function? When is it commonly used?\\n\\n\\nThe softmax activation function is used in neural networks to convert raw output scores (logits) into probabilities that sum up to 1. This is particularly useful for classification tasks.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "\n",
    "The softmax activation function is used in neural networks to convert raw output scores (logits) into probabilities that sum up to 1. This is particularly useful for classification tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\\n\\nThe hyperbolic tangent (tanh) activation function is another popular activation function used in neural networks. It is particularly known for its zero-centered output, which can provide some advantages over the sigmoid activation function.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "The hyperbolic tangent (tanh) activation function is another popular activation function used in neural networks. It is particularly known for its zero-centered output, which can provide some advantages over the sigmoid activation function.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git_pw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
