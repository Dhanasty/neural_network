{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1) Optimizers in machine learning are tools that guide a model in improving its predictions by adjusting internal parameters \\ncalled \"weights.\" The process begins with an initial guess, where the model makes predictions, and the optimizer compares these \\npredictions to the actual results, calculating how far off they are (called the \"error\" or \"loss\"). Based on this error, the optimizer\\n makes small adjustments to the model\\'s weights, aiming to reduce the error over time. The size of these adjustments is controlled by \\n a setting called the \"learning rate.\" This process repeats multiple times, with the optimizer continually fine-tuning the model\\'s \\n parameters to make it more accurate.\\n In essence, optimizers help a model learn better by refining its guesses step by step until it can make reliable predictions.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "1) Optimizers in machine learning are tools that guide a model in improving its predictions by adjusting internal parameters \n",
    "called \"weights.\" The process begins with an initial guess, where the model makes predictions, and the optimizer compares these \n",
    "predictions to the actual results, calculating how far off they are (called the \"error\" or \"loss\"). Based on this error, the optimizer\n",
    " makes small adjustments to the model's weights, aiming to reduce the error over time. The size of these adjustments is controlled by \n",
    " a setting called the \"learning rate.\" This process repeats multiple times, with the optimizer continually fine-tuning the model's \n",
    " parameters to make it more accurate.\n",
    " In essence, optimizers help a model learn better by refining its guesses step by step until it can make reliable predictions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n2) Gradient descent is an algorithm which helps the model to reduce it's error rate in the ml models. the core idea of this is to find the minimum point \\nin the gradient descent curve.\\n\\nGradient : tell's us the steepness of the curve and direction of the hill, to minimize the error you need to move downhill.\\n\\nLearning Rate : how large the steps move towards the minimum\\n\\nIterative process : starts with the random weights, gd repeatedly calculates the gradient and adjusts the weight\\n\\n\\nBatch GD :\\n    calculates the gradient of the entire dataset at once and then updates the weights based on the calc.\\n\\n    memory is high cause it calculates all at once,\\n    the speed is also slow\\n\\nStochastic GD :\\n    Sgd calculates gradient for one datapoint and updates the weight each time.\\n\\n    memeory is very low \\n    it is very fast\\n\\nMini Batch Gradient's :\\n    it processes small random batches of data, updating the weights after computing each batch\\n\\n    Memory and speed are moderate\\n\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2) Gradient descent is an algorithm which helps the model to reduce it's error rate in the ml models. the core idea of this is to find the minimum point \n",
    "in the gradient descent curve.\n",
    "\n",
    "Gradient : tell's us the steepness of the curve and direction of the hill, to minimize the error you need to move downhill.\n",
    "\n",
    "Learning Rate : how large the steps move towards the minimum\n",
    "\n",
    "Iterative process : starts with the random weights, gd repeatedly calculates the gradient and adjusts the weight\n",
    "\n",
    "\n",
    "Batch GD :\n",
    "    calculates the gradient of the entire dataset at once and then updates the weights based on the calc.\n",
    "\n",
    "    memory is high cause it calculates all at once,\n",
    "    the speed is also slow\n",
    "\n",
    "Stochastic GD :\n",
    "    Sgd calculates gradient for one datapoint and updates the weight each time.\n",
    "\n",
    "    memeory is very low \n",
    "    it is very fast\n",
    "\n",
    "Mini Batch Gradient's :\n",
    "    it processes small random batches of data, updating the weights after computing each batch\n",
    "\n",
    "    Memory and speed are moderate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n3) Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow\\nconvergence, local minima). How do modern optimizers address these challenges\\n\\nMomentum adjust's the slow convergence rate and oscillations\\nAdaptive Learning Rates: These optimizers adjust the learning rate frequently. if a parameter has high gradient then the learning rate will be minimum\\nwhen there is low gradient then learning rate will be maximum\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3) Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow\n",
    "convergence, local minima). How do modern optimizers address these challenges\n",
    "\n",
    "Momentum adjust's the slow convergence rate and oscillations\n",
    "Adaptive Learning Rates: These optimizers adjust the learning rate frequently. if a parameter has high gradient then the learning rate will be minimum\n",
    "when there is low gradient then learning rate will be maximum\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4) Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do\\nthey impact convergence and model performance?\\n\\nMomentum: it is a technique to accelearte the convergence and smooth out the learning process   \\n\\nLearning Rate : it determines the how quickly or slowly the model adjusts to the error during each iteration of the optimization algorithm\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4) Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do\n",
    "they impact convergence and model performance?\n",
    "\n",
    "Momentum: it is a technique to accelearte the convergence and smooth out the learning process   \n",
    "\n",
    "Learning Rate : it determines the how quickly or slowly the model adjusts to the error during each iteration of the optimization algorithm\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5) Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional\\ngradient descent. Discuss its limitations and scenarios where it is most suitablen \\n\\nSGD is an optimiztion technique that updates model in single data of every batch ,making it faster and memeory efficient\\nit helps in large data sets\\nSGD is suitable for online learning and real time application\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "5) Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional\n",
    "gradient descent. Discuss its limitations and scenarios where it is most suitablen \n",
    "\n",
    "SGD is an optimiztion technique that updates model in single data of every batch ,making it faster and memeory efficient\n",
    "it helps in large data sets\n",
    "SGD is suitable for online learning and real time application\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n6)\\n\\nThe Adam optimizer (Adaptive Moment Estimation) is a popular optimization algorithm that combines the benefits of momentum and adaptive learning rates. It calculates both the exponentially decaying average of past gradients (like momentum) and the exponentially decaying average of squared gradients (adaptive learning rates) for each parameter. The momentum aspect helps speed up learning by smoothing out fluctuations in the gradient, while the adaptive learning rate adjusts the step size for each parameter individually, allowing faster learning in some areas and more careful updates in others.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "6)\n",
    "\n",
    "The Adam optimizer (Adaptive Moment Estimation) is a popular optimization algorithm that combines the benefits of momentum and adaptive learning rates. It calculates both the exponentially decaying average of past gradients (like momentum) and the exponentially decaying average of squared gradients (adaptive learning rates) for each parameter. The momentum aspect helps speed up learning by smoothing out fluctuations in the gradient, while the adaptive learning rate adjusts the step size for each parameter individually, allowing faster learning in some areas and more careful updates in others.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git_pw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
